{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fcoel\\miniconda3\\envs\\apc-project\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os, glob, pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn_image as isns\n",
    "from PIL import Image\n",
    "\n",
    "import IPython"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description\n",
    "\n",
    "## Goal\n",
    "\n",
    "Using the embeddings, we will train many models, score them using selected metrics, and record all the results.\n",
    "\n",
    "Then we will build an ensemble with the best performing models.\n",
    "\n",
    "## Methods\n",
    "\n",
    "For metrics, we will consider the \n",
    "\n",
    "- F1-score (per-class and average)\n",
    "- Accuracy (per-class)\n",
    "- Balanced Accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../models/embedding_train.pickle\", \"rb\") as handle:\n",
    "    train_feats = pickle.load(handle)\n",
    "\n",
    "with open(\"../models/embedding_validation.pickle\", \"rb\") as handle:\n",
    "    validation_feats = pickle.load(handle)\n",
    "\n",
    "with open(\"../models/labels.pickle\", \"rb\") as handle:\n",
    "    labels_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = labels_dict[\"train\"]\n",
    "validation_labels = labels_dict[\"validation\"]\n",
    "categories = labels_dict[\"categorical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14034, 2048) (14034,)\n"
     ]
    }
   ],
   "source": [
    "print(train_feats.shape, train_labels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose models and set GridSearch\n",
    "\n",
    "- k-Nearest Neighbors\n",
    "- Decision Trees\n",
    "- Naive Bayes\n",
    "- Random Forest\n",
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from scipy.stats import randint, norm, uniform\n",
    "\n",
    "##########\n",
    "kNN = KNeighborsClassifier()\n",
    "\n",
    "kNN_params = {\n",
    "    \"n_neighbors\": [5, 10, 15, 20, 30]\n",
    "}\n",
    "##########\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "decision_tree_params = {}\n",
    "##########\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "random_forest_params = {\n",
    "    \"n_estimators\": [100, 150, 200, 250],\n",
    "}\n",
    "##########\n",
    "sgd_hinge = SGDClassifier()\n",
    "\n",
    "sgd_hinge_params = {\n",
    "    \"loss\": [\"hinge\"],\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "##########\n",
    "sgd_log = SGDClassifier()\n",
    "\n",
    "sgd_log_params = {\n",
    "    \"loss\": [\"log_loss\"],\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "##########\n",
    "sgd_huber = SGDClassifier()\n",
    "\n",
    "sgd_huber_params = {\n",
    "    \"loss\": [\"modified_huber\"],\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "##########\n",
    "gaussianNB = GaussianNB()\n",
    "\n",
    "gaussianNB_params = {}\n",
    "##########\n",
    "adaboost = AdaBoostClassifier()\n",
    "\n",
    "adaboost_params = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.1, 1., 10.]\n",
    "}\n",
    "##########\n",
    "\n",
    "\n",
    "models = [kNN, decision_tree, random_forest,\n",
    "          sgd_hinge, sgd_log, sgd_huber,\n",
    "          gaussianNB, adaboost]\n",
    "\n",
    "model_params = [kNN_params, decision_tree_params, random_forest_params,\n",
    "                sgd_hinge_params, sgd_log_params, sgd_huber_params,\n",
    "                gaussianNB_params, adaboost_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV] END ......................................n_neighbors=5; total time=   3.0s\n",
      "[CV] END ......................................n_neighbors=5; total time=   2.8s\n",
      "[CV] END ......................................n_neighbors=5; total time=   2.5s\n",
      "[CV] END ......................................n_neighbors=5; total time=   2.4s\n",
      "[CV] END ......................................n_neighbors=5; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=10; total time=   2.4s\n",
      "[CV] END .....................................n_neighbors=10; total time=   2.4s\n",
      "[CV] END .....................................n_neighbors=10; total time=   2.3s\n",
      "[CV] END .....................................n_neighbors=10; total time=   2.3s\n",
      "[CV] END .....................................n_neighbors=10; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=15; total time=   2.3s\n",
      "[CV] END .....................................n_neighbors=15; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=15; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=15; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=15; total time=   2.3s\n",
      "[CV] END .....................................n_neighbors=20; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=20; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=20; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=20; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=20; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=30; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=30; total time=   2.5s\n",
      "[CV] END .....................................n_neighbors=30; total time=   2.3s\n",
      "[CV] END .....................................n_neighbors=30; total time=   2.2s\n",
      "[CV] END .....................................n_neighbors=30; total time=   2.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .................................................... total time=  25.9s\n",
      "[CV] END .................................................... total time=  23.8s\n",
      "[CV] END .................................................... total time=  26.9s\n",
      "[CV] END .................................................... total time=  25.5s\n",
      "[CV] END .................................................... total time=  24.5s\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] END ...................................n_estimators=100; total time=  23.7s\n",
      "[CV] END ...................................n_estimators=100; total time=  24.2s\n",
      "[CV] END ...................................n_estimators=100; total time=  23.6s\n",
      "[CV] END ...................................n_estimators=100; total time=  23.2s\n",
      "[CV] END ...................................n_estimators=100; total time=  23.0s\n",
      "[CV] END ...................................n_estimators=150; total time=  34.5s\n",
      "[CV] END ...................................n_estimators=150; total time=  34.9s\n",
      "[CV] END ...................................n_estimators=150; total time=  34.1s\n",
      "[CV] END ...................................n_estimators=150; total time=  34.6s\n",
      "[CV] END ...................................n_estimators=150; total time=  34.5s\n",
      "[CV] END ...................................n_estimators=200; total time=  46.9s\n",
      "[CV] END ...................................n_estimators=200; total time=  46.6s\n",
      "[CV] END ...................................n_estimators=200; total time=  46.4s\n",
      "[CV] END ...................................n_estimators=200; total time=  46.6s\n",
      "[CV] END ...................................n_estimators=200; total time=  46.6s\n",
      "[CV] END ...................................n_estimators=250; total time=  57.8s\n",
      "[CV] END ...................................n_estimators=250; total time=  58.2s\n",
      "[CV] END ...................................n_estimators=250; total time=  58.7s\n",
      "[CV] END ...................................n_estimators=250; total time=  58.1s\n",
      "[CV] END ...................................n_estimators=250; total time=  58.8s\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] END ...........................alpha=0.0001, loss=hinge; total time=  12.6s\n",
      "[CV] END ...........................alpha=0.0001, loss=hinge; total time=   9.4s\n",
      "[CV] END ...........................alpha=0.0001, loss=hinge; total time=   9.9s\n",
      "[CV] END ...........................alpha=0.0001, loss=hinge; total time=  11.2s\n",
      "[CV] END ...........................alpha=0.0001, loss=hinge; total time=  10.6s\n",
      "[CV] END ............................alpha=0.001, loss=hinge; total time=   8.7s\n",
      "[CV] END ............................alpha=0.001, loss=hinge; total time=   8.4s\n",
      "[CV] END ............................alpha=0.001, loss=hinge; total time=   9.2s\n",
      "[CV] END ............................alpha=0.001, loss=hinge; total time=   8.3s\n",
      "[CV] END ............................alpha=0.001, loss=hinge; total time=   9.4s\n",
      "[CV] END .............................alpha=0.01, loss=hinge; total time=   4.1s\n",
      "[CV] END .............................alpha=0.01, loss=hinge; total time=   4.6s\n",
      "[CV] END .............................alpha=0.01, loss=hinge; total time=   4.7s\n",
      "[CV] END .............................alpha=0.01, loss=hinge; total time=   4.5s\n",
      "[CV] END .............................alpha=0.01, loss=hinge; total time=   4.6s\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] END ........................alpha=0.0001, loss=log_loss; total time=  22.3s\n",
      "[CV] END ........................alpha=0.0001, loss=log_loss; total time=  20.8s\n",
      "[CV] END ........................alpha=0.0001, loss=log_loss; total time=  19.1s\n",
      "[CV] END ........................alpha=0.0001, loss=log_loss; total time=  23.0s\n",
      "[CV] END ........................alpha=0.0001, loss=log_loss; total time=  22.3s\n",
      "[CV] END .........................alpha=0.001, loss=log_loss; total time=  11.9s\n",
      "[CV] END .........................alpha=0.001, loss=log_loss; total time=  11.2s\n",
      "[CV] END .........................alpha=0.001, loss=log_loss; total time=  10.7s\n",
      "[CV] END .........................alpha=0.001, loss=log_loss; total time=  11.3s\n",
      "[CV] END .........................alpha=0.001, loss=log_loss; total time=  12.1s\n",
      "[CV] END ..........................alpha=0.01, loss=log_loss; total time=   5.1s\n",
      "[CV] END ..........................alpha=0.01, loss=log_loss; total time=   5.7s\n",
      "[CV] END ..........................alpha=0.01, loss=log_loss; total time=   4.7s\n",
      "[CV] END ..........................alpha=0.01, loss=log_loss; total time=   4.5s\n",
      "[CV] END ..........................alpha=0.01, loss=log_loss; total time=   4.9s\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV] END ..................alpha=0.0001, loss=modified_huber; total time=  12.3s\n",
      "[CV] END ..................alpha=0.0001, loss=modified_huber; total time=  11.2s\n",
      "[CV] END ..................alpha=0.0001, loss=modified_huber; total time=  12.2s\n",
      "[CV] END ..................alpha=0.0001, loss=modified_huber; total time=  13.1s\n",
      "[CV] END ..................alpha=0.0001, loss=modified_huber; total time=  12.4s\n",
      "[CV] END ...................alpha=0.001, loss=modified_huber; total time=  12.4s\n",
      "[CV] END ...................alpha=0.001, loss=modified_huber; total time=  10.1s\n",
      "[CV] END ...................alpha=0.001, loss=modified_huber; total time=  12.5s\n",
      "[CV] END ...................alpha=0.001, loss=modified_huber; total time=  11.1s\n",
      "[CV] END ...................alpha=0.001, loss=modified_huber; total time=  11.8s\n",
      "[CV] END ....................alpha=0.01, loss=modified_huber; total time=   9.7s\n",
      "[CV] END ....................alpha=0.01, loss=modified_huber; total time=   7.6s\n",
      "[CV] END ....................alpha=0.01, loss=modified_huber; total time=   8.1s\n",
      "[CV] END ....................alpha=0.01, loss=modified_huber; total time=   7.7s\n",
      "[CV] END ....................alpha=0.01, loss=modified_huber; total time=   8.6s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .................................................... total time=   0.6s\n",
      "[CV] END .................................................... total time=   0.5s\n",
      "[CV] END .................................................... total time=   0.5s\n",
      "[CV] END .................................................... total time=   0.6s\n",
      "[CV] END .................................................... total time=   0.5s\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time= 1.4min\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time= 1.3min\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time= 1.4min\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time= 1.4min\n",
      "[CV] END .................learning_rate=0.1, n_estimators=50; total time= 1.4min\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time= 2.3min\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=0.1, n_estimators=100; total time= 2.2min\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time= 1.1min\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time= 1.1min\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time= 1.1min\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time= 1.1min\n",
      "[CV] END .................learning_rate=1.0, n_estimators=50; total time= 1.1min\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=1.0, n_estimators=100; total time= 2.2min\n",
      "[CV] END ................learning_rate=10.0, n_estimators=50; total time=  32.5s\n",
      "[CV] END ................learning_rate=10.0, n_estimators=50; total time=  27.5s\n",
      "[CV] END ................learning_rate=10.0, n_estimators=50; total time=  32.5s\n",
      "[CV] END ................learning_rate=10.0, n_estimators=50; total time=  31.9s\n",
      "[CV] END ................learning_rate=10.0, n_estimators=50; total time=  27.0s\n",
      "[CV] END ...............learning_rate=10.0, n_estimators=100; total time=  30.1s\n",
      "[CV] END ...............learning_rate=10.0, n_estimators=100; total time=  28.8s\n",
      "[CV] END ...............learning_rate=10.0, n_estimators=100; total time=  33.6s\n",
      "[CV] END ...............learning_rate=10.0, n_estimators=100; total time=  25.0s\n",
      "[CV] END ...............learning_rate=10.0, n_estimators=100; total time=  41.5s\n"
     ]
    }
   ],
   "source": [
    "best_models = []\n",
    "best_params = []\n",
    "best_scores = []\n",
    "\n",
    "for model, param in zip(models, model_params):\n",
    "\n",
    "    cv = GridSearchCV(model, param, scoring=\"balanced_accuracy\", verbose=2)\n",
    "    search = cv.fit(train_feats, train_labels)\n",
    "\n",
    "    best_models.append(search.best_estimator_)\n",
    "    best_params.append(search.best_params_)\n",
    "    best_scores.append(search.best_score_)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=20), score: 85.894450\n",
      "DecisionTreeClassifier(), score: 75.588079\n",
      "RandomForestClassifier(n_estimators=250), score: 87.855905\n",
      "SGDClassifier(alpha=0.01), score: 88.989602\n",
      "SGDClassifier(alpha=0.01, loss='log_loss'), score: 88.847057\n",
      "SGDClassifier(alpha=0.01, loss='modified_huber'), score: 87.707370\n",
      "GaussianNB(), score: 75.014220\n",
      "AdaBoostClassifier(learning_rate=0.1, n_estimators=100), score: 77.550139\n"
     ]
    }
   ],
   "source": [
    "for model, score in zip(best_models, best_scores):\n",
    "    print(f\"{model}, score: {score*100:1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_results = {\"models\": best_models, \"scores\": best_scores, \"params\": best_params}\n",
    "\n",
    "# with open(\"../models/classifiers.pickle\", mode=\"wb\") as handle:\n",
    "#     pickle.dump(my_results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../models/classifiers.pickle\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m----> 2\u001b[0m     results \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(handle)\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# with open(\"../models/classifiers.pickle\", mode=\"rb\") as handle:\n",
    "#     results = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "balAcc = []\n",
    "f1Score = []\n",
    "\n",
    "for model in best_models:\n",
    "    preds = model.predict(validation_feats)\n",
    "    f1Score.append(f1_score(validation_labels, preds, average=\"micro\"))\n",
    "    balAcc.append(balanced_accuracy_score(validation_labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8649622670880048, 0.7463760786866117, 0.876061337397413, 0.9046821209329184, 0.907755037819561, 0.8855345257895495, 0.7393969206966595, 0.7831770779748322]\n"
     ]
    }
   ],
   "source": [
    "print(balAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fcoel\\miniconda3\\envs\\apc-project\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:47:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"num_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:47:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:47:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"num_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:47:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:48:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"num_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:48:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:49:30] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"num_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:49:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:50:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"num_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:50:19] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "xgboost = XGBClassifier(objective=\"multi:softmax\", num_class=6, num_estimators=100, max_depth=2, use_label_encoder=False)\n",
    "\n",
    "xgboost_scores = cross_val_score(xgboost, train_feats, train_labels, scoring=\"balanced_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.81938479861999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xgboost_scores.max())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:53:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"num_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:53:47] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgboost.fit(train_feats, train_labels)\n",
    "preds = xgboost.predict(validation_feats)\n",
    "val_scores = balanced_accuracy_score(validation_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apc-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b6fecf262674c92f03e47c8fc19f0255723842d1c75d45d212a216e59554918"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
